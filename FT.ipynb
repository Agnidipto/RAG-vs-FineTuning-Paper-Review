{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8c39f9fd2e4d4b29bdc459161c170495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e776826a4ba9400195551930bcedc847",
              "IPY_MODEL_2202a612e250447aa1ad1646d33ba4ff",
              "IPY_MODEL_c5692c5e38aa44ed9bc00c683dcd5719"
            ],
            "layout": "IPY_MODEL_f2d694c05b3a41dd9a68ef09fe11da1e"
          }
        },
        "e776826a4ba9400195551930bcedc847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f27f1a2f7d24490bf8b0c8b95ab25d0",
            "placeholder": "​",
            "style": "IPY_MODEL_562294f9ebcd4d7197a0773b4d2c0d39",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2202a612e250447aa1ad1646d33ba4ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_058c595b554649458f0f4f55057f71c3",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4de4e4838e444ad5bcd42bc5cd044c4b",
            "value": 3
          }
        },
        "c5692c5e38aa44ed9bc00c683dcd5719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f9b723f99234aa09c957054f217acc5",
            "placeholder": "​",
            "style": "IPY_MODEL_3f662c4a2a1f420ba646eded3f6b6331",
            "value": " 3/3 [01:37&lt;00:00, 32.29s/it]"
          }
        },
        "f2d694c05b3a41dd9a68ef09fe11da1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f27f1a2f7d24490bf8b0c8b95ab25d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "562294f9ebcd4d7197a0773b4d2c0d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "058c595b554649458f0f4f55057f71c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4de4e4838e444ad5bcd42bc5cd044c4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f9b723f99234aa09c957054f217acc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f662c4a2a1f420ba646eded3f6b6331": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTNkReZhOkVq",
        "outputId": "20db5578-6d65-415a-b661-4e0d18498f35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import shutil\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from peft import PeftModel"
      ],
      "metadata": {
        "id": "XKkSgSX4PNsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ze5l82XGPQl4",
        "outputId": "d6028313-46fc-48e2-e725-1b83d3ec1cc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_folder = '/content/drive/MyDrive/NLP_Paper_Review/dataset'\n",
        "destination_folder = '/content/dataset'\n",
        "\n",
        "if not os.path.exists(destination_folder):\n",
        "    os.makedirs(destination_folder)\n",
        "\n",
        "# Copy the folder and its contents\n",
        "shutil.copytree(source_folder, destination_folder, dirs_exist_ok=True)\n",
        "\n",
        "print(f\"Files copied from {source_folder} to {destination_folder}\")\n",
        "\n",
        "source_path = '/content/drive/MyDrive/NLP_Paper_Review/ft-mistral.zip'\n",
        "destination_path = '/content/ft-mistral.zip'\n",
        "\n",
        "# Copy the file\n",
        "shutil.copy(source_path, destination_path)\n",
        "print(f\"Model copied from {source_path} to {destination_path}\")\n",
        "\n",
        "# Extract the model\n",
        "import zipfile\n",
        "with zipfile.ZipFile(destination_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('extracted_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzam429jPXeR",
        "outputId": "603b28fc-80c8-453c-8e8d-074739e80faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files copied from /content/drive/MyDrive/NLP_Paper_Review/dataset to /content/dataset\n",
            "Model copied from /content/drive/MyDrive/NLP_Paper_Review/ft-mistral.zip to /content/ft-mistral.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "# Load the model from the extracted directory\n",
        "\n",
        "BASE_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"        # original weights\n",
        "\n",
        "bnb_conf = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,            # change to float16 if bf16 unsupported\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "        quantization_config=bnb_conf,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map={\"\": 0},\n",
        "        trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# ❷  attach your LoRA deltas\n",
        "model = PeftModel.from_pretrained(base, \"/content/extracted_model/ft-mistral\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_ID, use_fast=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8c39f9fd2e4d4b29bdc459161c170495",
            "e776826a4ba9400195551930bcedc847",
            "2202a612e250447aa1ad1646d33ba4ff",
            "c5692c5e38aa44ed9bc00c683dcd5719",
            "f2d694c05b3a41dd9a68ef09fe11da1e",
            "4f27f1a2f7d24490bf8b0c8b95ab25d0",
            "562294f9ebcd4d7197a0773b4d2c0d39",
            "058c595b554649458f0f4f55057f71c3",
            "4de4e4838e444ad5bcd42bc5cd044c4b",
            "2f9b723f99234aa09c957054f217acc5",
            "3f662c4a2a1f420ba646eded3f6b6331"
          ]
        },
        "id": "ZSd0sK_oPwKS",
        "outputId": "7d29bd0b-9857-4808-b241-d098d0ad5f2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c39f9fd2e4d4b29bdc459161c170495"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(question, options):\n",
        "\n",
        "    # Format options\n",
        "    options_str = \"\\n\".join([f\"{option}\" for ind, option in enumerate(options)])\n",
        "\n",
        "    instructions = \"Just provide the answer to the Question at the end of the prompt based on the sample context questions provided. Mention the option number ONLY. Use Numbers for options, not anything else.\"\n",
        "\n",
        "    instructions = \"\"\n",
        "\n",
        "    augmented_prompt = f\"{instructions}\\n\\n{question}\\nOptions:\\n{options_str}\\n\\nAnswer:\"\n",
        "\n",
        "    # Generate response - move inputs to the same device as model\n",
        "    inputs = tokenizer(augmented_prompt, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}  # Move inputs to model's device\n",
        "\n",
        "    # Generate only new tokens (the answer)\n",
        "    prompt_length = inputs[\"input_ids\"].shape[1]\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Extract only the newly generated tokens (the answer part)\n",
        "    answer_tokens = outputs[0][prompt_length:]\n",
        "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
        "    # return answer.strip()\n",
        "    answer_option = answer.strip().split('\\n')[0]\n",
        "    return answer_option"
      ],
      "metadata": {
        "id": "tHh_JyedQXux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How have we been able to construct detailed maps of surface features on Venus?\"\n",
        "options = [\"by studying Venus from Earth with powerful optical telescopes\",\"by landing spacecraft on the surface for close-up study\",\"by studying Venus with powerful optical telescopes on spacecraft that were sent to orbit Venus\",\"by using radar from spacecraft that were sent to orbit Venus\"]\n",
        "response = predict(query, options)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOixt8YER5ZL",
        "outputId": "c6d40a1c-a6c2-4fa4-d404-28f64b447837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "by using radar from spacecraft that were sent to orbit Venus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "st_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "def get_pred_indexes(pred, dataset) :\n",
        "  pred_indexes = []\n",
        "  for ind, pred in enumerate(pred) :\n",
        "    choices = dataset[ind]['choices']\n",
        "    pred_embedding = st_model.encode([pred], normalize_embeddings=True)\n",
        "    choice_embeddings = st_model.encode(choices, normalize_embeddings=True)\n",
        "    similarities = cosine_similarity(pred_embedding, choice_embeddings)[0]\n",
        "    # Find the most similar sentence\n",
        "    most_similar_idx = int(np.argmax(similarities))\n",
        "    pred_indexes.append(most_similar_idx)\n",
        "  return pred_indexes"
      ],
      "metadata": {
        "id": "CyLOYI3gWXvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation = json.load(open('dataset/mmlu_datasets/mmlu_astronomy_validation.json'))\n",
        "test = json.load(open('dataset/mmlu_datasets/mmlu_astronomy_test.json'))\n",
        "\n",
        "validation[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzieU8oFR78U",
        "outputId": "28068c3f-4caa-474b-bd30-dfbf193ffaa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'You cool a blackbody to half its original temperature. How does its spectrum change?',\n",
              " 'subject': 'astronomy',\n",
              " 'choices': ['Power emitted is 1/16 times as high; peak emission wavelength is 1/2 as long.',\n",
              "  'Power emitted is 1/4 times as high; peak emission wavelength is 2 times longer.',\n",
              "  'Power emitted is 1/4 times as high; peak emission wavelength is 1/2 as long.',\n",
              "  'Power emitted is 1/16 times as high; peak emission wavelength is 2 times longer.'],\n",
              " 'answer': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred = []\n",
        "val_actual = [item['answer'] for item in validation]\n",
        "for item in tqdm(validation, desc='Validating'):\n",
        "  pred = predict(item['question'], item['choices'])\n",
        "  val_pred.append(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uWdPy-lSK2D",
        "outputId": "086cc361-a90b-47fa-88ff-a0b2a7a7f89f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rValidating:   0%|          | 0/16 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Validating: 100%|██████████| 16/16 [01:06<00:00,  4.13s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred[:5], val_actual[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDjWKZDzSSr-",
        "outputId": "04a8dd44-2c8d-422a-f70f-bcf2d92e5581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Power emitted is 1/4 times as high; peak emission wavelength is 1/2 as long.',\n",
              "  'The minimization of gravitational potential energy.',\n",
              "  \"Large impacts fractured the Moon's lithosphere allowing lava to fill the impact basins.\",\n",
              "  'It does not rotate fast enough.',\n",
              "  '5 : 1'],\n",
              " [3, 1, 3, 0, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred_indexes = get_pred_indexes(val_pred, validation)\n",
        "val_pred_indexes, val_actual"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwswOSQ8Urqk",
        "outputId": "d5094868-42b7-44cf-f05f-b181f5c9ab1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([2, 1, 3, 2, 2, 2, 3, 2, 2, 1, 1, 0, 2, 1, 3, 1],\n",
              " [3, 1, 3, 0, 2, 1, 3, 3, 0, 0, 1, 0, 2, 1, 3, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = (np.array(val_pred_indexes) == np.array(val_actual)).mean()\n",
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueX-WB3BX7MG",
        "outputId": "3c73941f-0535-45e7-e50a-c047f78cd6fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.5625)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = []\n",
        "for ind, item in enumerate(validation):\n",
        "    # Extract the question, choices, and answer index\n",
        "    question = item['question']\n",
        "    choices = item['choices']\n",
        "    answer_idx = item['answer']\n",
        "\n",
        "    # Create a dictionary for this row\n",
        "    row = {\n",
        "        'question': question,\n",
        "        'option_0': choices[0],\n",
        "        'option_1': choices[1],\n",
        "        'option_2': choices[2],\n",
        "        'option_3': choices[3],\n",
        "        'answer_idx': answer_idx,\n",
        "        'predicted_idx': val_pred_indexes[ind]\n",
        "    }\n",
        "\n",
        "    # Add to our data list\n",
        "    data.append(row)\n",
        "df = pd.DataFrame(data)\n",
        "df.head()\n",
        "df.to_csv('ft_validation_results.csv', index=False)"
      ],
      "metadata": {
        "id": "d5bVoC8eSpa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = []\n",
        "test_actual = [item['answer'] for item in test]\n",
        "for item in tqdm(test, desc=\"Testing\"):\n",
        "  pred = predict(item['question'], item['choices'])\n",
        "  test_pred.append(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KadBeAHaUj80",
        "outputId": "d4a5368f-998c-4ca7-beb4-aa83e5836e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTesting:   0%|          | 0/152 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Testing: 100%|██████████| 152/152 [11:16<00:00,  4.45s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred_indexes = get_pred_indexes(test_pred, test)\n",
        "test_pred_indexes[0:5], test_actual[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkplAkOWYepG",
        "outputId": "f15e86e5-50de-4c64-921b-70bf3d957a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 3, 2, 3, 3], [0, 3, 2, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = (np.array(test_pred_indexes) == np.array(test_actual)).mean()\n",
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdWbaM7yd3wa",
        "outputId": "2e0e87bd-4988-4e0d-a9ce-4f311d541948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.6842105263157895)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for ind, item in enumerate(test):\n",
        "    # Extract the question, choices, and answer index\n",
        "    question = item['question']\n",
        "    choices = item['choices']\n",
        "    answer_idx = item['answer']\n",
        "\n",
        "    # Create a dictionary for this row\n",
        "    row = {\n",
        "        'question': question,\n",
        "        'option_0': choices[0],\n",
        "        'option_1': choices[1],\n",
        "        'option_2': choices[2],\n",
        "        'option_3': choices[3],\n",
        "        'answer_idx': answer_idx,\n",
        "        'predicted_idx': test_pred_indexes[ind]\n",
        "    }\n",
        "\n",
        "    # Add to our data list\n",
        "    data.append(row)\n",
        "df = pd.DataFrame(data)\n",
        "df.head()\n",
        "df.to_csv('ft_test_results.csv', index=False)"
      ],
      "metadata": {
        "id": "o2r1010_eOL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG + FT"
      ],
      "metadata": {
        "id": "t4C_VjbWfcLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "with open('dataset/training_data.json', 'r') as f:\n",
        "  train_data: list[dict] = json.load(f)\n",
        "\n",
        "# with open('dataset/astronomy_augmented_1.json', 'r') as f:\n",
        "#   augmented_data_1: list[dict] = json.load(f)\n",
        "\n",
        "with open('dataset/astronomy_augmented_2.json', 'r', encoding='UTF-8') as f:\n",
        "  augmented_data_2: list[dict] = json.load(f)\n",
        "\n",
        "data += train_data + augmented_data_2 # + augmented_data_1\n",
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhyhqR_ZeZLf",
        "outputId": "25f61490-1e48-4573-9fac-2d4aba65db1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3472"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = []\n",
        "for item in data :\n",
        "  question = item['question']\n",
        "  choices = item['choices']\n",
        "  answer_idx = item['answer']\n",
        "  answer = choices[answer_idx]\n",
        "\n",
        "  chunk = f'Question: {question} \\nAnswer: {answer}'\n",
        "  chunks.append(chunk)\n",
        "\n",
        "chunks[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEGEMAnrfypP",
        "outputId": "b76c2167-3e8c-47f0-8343-db8d23ab0ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Question: According to the article, what is an asteroid? \\nAnswer: An object larger than a meteoroid that is neither a planet nor an identified comet, orbiting within the inner Solar System or co-orbital with Jupiter',\n",
              " 'Question: What are the broad classifications of asteroids based on composition? \\nAnswer: Carbonaceous, Metallic, Silicaceous',\n",
              " 'Question: Where are the greatest number of known asteroids located? \\nAnswer: Between the orbits of Mars and Jupiter',\n",
              " 'Question: Which of the following spacecraft directly studied Vesta and Ceres? \\nAnswer: Dawn',\n",
              " 'Question: What potentially catastrophic event is associated with Near-Earth asteroids? \\nAnswer: Colliding with Earth and causing mass extinction events']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer(\"BAAI/bge-large-en\")\n",
        "chunk_embeddings = embedding_model.encode(chunks, normalize_embeddings=True)"
      ],
      "metadata": {
        "id": "eIIUqDwXgSNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dimension = chunk_embeddings.shape[1]\n",
        "dimension"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQajcbcBgmWj",
        "outputId": "fbc867f7-8fbb-45a1-a2f1-3de5a15fd6ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = faiss.IndexFlatIP(dimension)\n",
        "index.add(chunk_embeddings)"
      ],
      "metadata": {
        "id": "4glZ0Ei5grxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag(question, options, k=3):\n",
        "    # Generate query embedding and normalize\n",
        "    question_embedding = embedding_model.encode([question], normalize_embeddings=True)\n",
        "\n",
        "    # Retrieve top-k similar chunks\n",
        "    scores, indices = index.search(question_embedding, k)\n",
        "\n",
        "    # Construct context from retrieved chunks\n",
        "    context = \"\\n\\n\".join([chunks[idx] for idx in indices[0]])\n",
        "\n",
        "    # Format options\n",
        "    options_str = \"\\n\".join([f\"{option}\" for ind, option in enumerate(options)])\n",
        "\n",
        "    instructions = \"Just provide the answer to the Question at the end of the prompt based on the sample context questions provided. \"\n",
        "\n",
        "    # Create prompt\n",
        "    augmented_prompt = f\"{instructions}\\n\\n-----Beginning of Context------:\\n{context}\\n------End of Context-------\\n\\n\\nQuestion to answer: {question}\\nOptions to choose from:\\n{options_str}\\n\\nAnswer:\"\n",
        "\n",
        "    # Generate response - move inputs to the same device as model\n",
        "    inputs = tokenizer(augmented_prompt, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}  # Move inputs to model's device\n",
        "\n",
        "    # Generate only new tokens (the answer)\n",
        "    prompt_length = inputs[\"input_ids\"].shape[1]\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Extract only the newly generated tokens (the answer part)\n",
        "    answer_tokens = outputs[0][prompt_length:]\n",
        "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
        "    answer_option = answer.strip().split('\\n')[0]\n",
        "    return answer_option"
      ],
      "metadata": {
        "id": "MiHI2Mcvf17n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How have we been able to construct detailed maps of surface features on Venus?\"\n",
        "options = [\"by studying Venus from Earth with powerful optical telescopes\",\"by landing spacecraft on the surface for close-up study\",\"by studying Venus with powerful optical telescopes on spacecraft that were sent to orbit Venus\",\"by using radar from spacecraft that were sent to orbit Venus\"]\n",
        "response = rag(query, options)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXxvMk9Zg2iO",
        "outputId": "3d8ebaa1-eec8-428e-d050-92df5deb625c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "by using radar from spacecraft that were sent to orbit Venus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "val_pred = []\n",
        "val_actual = [item['answer'] for item in validation]\n",
        "\n",
        "# Create progress bar\n",
        "for item in tqdm(validation, desc=\"Validating\"):\n",
        "    pred = rag(item['question'], item['choices'])\n",
        "    val_pred.append(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBnz9Wiug6XG",
        "outputId": "65a408b4-6d4c-4bc5-ba87-b1243c974020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 16/16 [01:33<00:00,  5.85s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hd7dGT1hTVB",
        "outputId": "9dd505e5-f105-4f47-fd0a-3cebfc0a4e68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Power emitted is 1/16 times as high; peak emission wavelength is 2 times longer.',\n",
              " 'The minimization of gravitational potential energy.',\n",
              " \"Large impacts fractured the Moon's lithosphere allowing lava to fill the impact basins.\",\n",
              " 'Its rotation axis is nearly perpendicular to the plane of the Solar System.',\n",
              " '5 : 1']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred_indexes = get_pred_indexes(val_pred, validation)\n",
        "val_pred_indexes, val_actual"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYhLCv_OjXOR",
        "outputId": "b8900dc9-e799-4057-fc83-d597be3c1577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([3, 1, 3, 0, 2, 1, 3, 3, 0, 0, 1, 0, 2, 1, 3, 0],\n",
              " [3, 1, 3, 0, 2, 1, 3, 3, 0, 0, 1, 0, 2, 1, 3, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = (np.array(val_pred_indexes) == np.array(val_actual)).mean()\n",
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6OUY-ITn62-",
        "outputId": "86818d91-c825-4dcc-918a-556ac061a742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for ind, item in enumerate(validation):\n",
        "    # Extract the question, choices, and answer index\n",
        "    question = item['question']\n",
        "    choices = item['choices']\n",
        "    answer_idx = item['answer']\n",
        "\n",
        "    # Create a dictionary for this row\n",
        "    row = {\n",
        "        'question': question,\n",
        "        'option_0': choices[0],\n",
        "        'option_1': choices[1],\n",
        "        'option_2': choices[2],\n",
        "        'option_3': choices[3],\n",
        "        'answer_idx': answer_idx,\n",
        "        'predicted_idx': val_pred_indexes[ind]\n",
        "    }\n",
        "\n",
        "    # Add to our data list\n",
        "    data.append(row)\n",
        "df = pd.DataFrame(data)\n",
        "df.head()\n",
        "df.to_csv('ft_rag_validation_results.csv', index=False)"
      ],
      "metadata": {
        "id": "2wELBNx6jbO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = []\n",
        "test_actual = [item['answer'] for item in test]\n",
        "for item in tqdm(test, desc=\"Testing\"):\n",
        "  pred = rag(item['question'], item['choices'])\n",
        "  test_pred.append(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8KXCsLBjv16",
        "outputId": "e3ff55ed-9728-4ebc-a530-e1f8cf1933ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTesting:   0%|          | 0/152 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Testing: 100%|██████████| 152/152 [14:16<00:00,  5.64s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred_indexes = get_pred_indexes(test_pred, test)\n",
        "test_pred_indexes[0:15], test_actual[0:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGbxj0y5kE0W",
        "outputId": "75384b45-e71a-4956-bb4a-322d04750ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 3, 2, 3, 3, 3, 3, 1, 3, 2, 3, 2, 0, 3, 1],\n",
              " [0, 3, 2, 2, 3, 2, 1, 1, 3, 3, 3, 2, 0, 3, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = (np.array(test_pred_indexes) == np.array(test_actual)).mean()\n",
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IozUD5GknyJ0",
        "outputId": "8eb1e5b9-8499-4577-ab17-938fffde6fd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.631578947368421)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for ind, item in enumerate(test):\n",
        "    # Extract the question, choices, and answer index\n",
        "    question = item['question']\n",
        "    choices = item['choices']\n",
        "    answer_idx = item['answer']\n",
        "\n",
        "    # Create a dictionary for this row\n",
        "    row = {\n",
        "        'question': question,\n",
        "        'option_0': choices[0],\n",
        "        'option_1': choices[1],\n",
        "        'option_2': choices[2],\n",
        "        'option_3': choices[3],\n",
        "        'answer_idx': answer_idx,\n",
        "        'predicted_idx': test_pred_indexes[ind]\n",
        "    }\n",
        "\n",
        "    # Add to our data list\n",
        "    data.append(row)\n",
        "df = pd.DataFrame(data)\n",
        "df.head()\n",
        "df.to_csv('ft_rag_test_results.csv', index=False)"
      ],
      "metadata": {
        "id": "chtz7nZRoEj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TrLZFFZ8oatQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}