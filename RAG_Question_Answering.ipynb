{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ijJkZNpSn-xx",
      "metadata": {
        "id": "ijJkZNpSn-xx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01adb60c-039e-469f-bad7-d265203de6a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.magic import register_cell_magic\n",
        "\n",
        "@register_cell_magic\n",
        "def skip(line, cell):\n",
        "    \"\"\"Skip execution of the current cell\"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "DD7AuH8OMoeA"
      },
      "id": "DD7AuH8OMoeA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f01ab5f1-6113-421e-a5a2-cc512cb8f0b4",
      "metadata": {
        "id": "f01ab5f1-6113-421e-a5a2-cc512cb8f0b4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import shutil\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UEZkcD-ioUYB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEZkcD-ioUYB",
        "outputId": "c3c86251-6db5-405c-826b-4c04689ca8b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XnkgkT1zomfZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnkgkT1zomfZ",
        "outputId": "073859b9-2d29-45fa-eceb-bb4087406f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files copied from /content/drive/MyDrive/NLP_Paper_Review/dataset to /content/dataset\n"
          ]
        }
      ],
      "source": [
        "source_folder = '/content/drive/MyDrive/NLP_Paper_Review/dataset'\n",
        "destination_folder = '/content/dataset'\n",
        "\n",
        "if not os.path.exists(destination_folder):\n",
        "    os.makedirs(destination_folder)\n",
        "\n",
        "# Copy the folder and its contents\n",
        "shutil.copytree(source_folder, destination_folder, dirs_exist_ok=True)\n",
        "\n",
        "print(f\"Files copied from {source_folder} to {destination_folder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b6400b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b6400b0",
        "outputId": "eea50720-34d4-4786-8498-8ef54f14ff35"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3472"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data = []\n",
        "with open('dataset/training_data.json', 'r') as f:\n",
        "  train_data: list[dict] = json.load(f)\n",
        "\n",
        "# with open('dataset/astronomy_augmented_1.json', 'r') as f:\n",
        "#   augmented_data_1: list[dict] = json.load(f)\n",
        "\n",
        "with open('dataset/astronomy_augmented_2.json', 'r', encoding='UTF-8') as f:\n",
        "  augmented_data_2: list[dict] = json.load(f)\n",
        "\n",
        "data += train_data + augmented_data_2 # + augmented_data_1\n",
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92931158",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92931158",
        "outputId": "99c04eaa-7871-45ff-8dd0-a7d60eab1847"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['question', 'choices', 'answer'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "data[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13e04986",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13e04986",
        "outputId": "27cea01b-8d5a-433e-e8ec-da93266b5d4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Question: According to the article, what is an asteroid? \\nAnswer: An object larger than a meteoroid that is neither a planet nor an identified comet, orbiting within the inner Solar System or co-orbital with Jupiter',\n",
              " 'Question: What are the broad classifications of asteroids based on composition? \\nAnswer: Carbonaceous, Metallic, Silicaceous',\n",
              " 'Question: Where are the greatest number of known asteroids located? \\nAnswer: Between the orbits of Mars and Jupiter',\n",
              " 'Question: Which of the following spacecraft directly studied Vesta and Ceres? \\nAnswer: Dawn',\n",
              " 'Question: What potentially catastrophic event is associated with Near-Earth asteroids? \\nAnswer: Colliding with Earth and causing mass extinction events']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "chunks = []\n",
        "for item in data :\n",
        "  question = item['question']\n",
        "  choices = item['choices']\n",
        "  answer_idx = item['answer']\n",
        "  answer = choices[answer_idx]\n",
        "\n",
        "  chunk = f'Question: {question} \\nAnswer: {answer}'\n",
        "  chunks.append(chunk)\n",
        "\n",
        "chunks[0:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5322fdf",
      "metadata": {
        "id": "a5322fdf"
      },
      "outputs": [],
      "source": [
        "embedding_model = SentenceTransformer(\"BAAI/bge-large-en\")\n",
        "chunk_embeddings = embedding_model.encode(chunks, normalize_embeddings=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05f90997",
      "metadata": {
        "id": "05f90997",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ff84581-4a3e-47c7-8099-a809a2686209"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "dimension = chunk_embeddings.shape[1]\n",
        "dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9df2c9fb",
      "metadata": {
        "id": "9df2c9fb"
      },
      "outputs": [],
      "source": [
        "index = faiss.IndexFlatIP(dimension)\n",
        "index.add(chunk_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AykPSK8h3_nE",
      "metadata": {
        "id": "AykPSK8h3_nE"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5de2f998",
      "metadata": {
        "id": "5de2f998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "452e0900bcb6419ab13dfda79bfc624e",
            "cc8bb6ddbfb84d04a1d1e232ab4af3d2",
            "6cff29d0e06446eaa3d4c8c80eafd8a5",
            "ae359486e20a4f8da832221b906c6679",
            "6308b1005afa4269b3c750ebbd15643d",
            "531a7b08732d43a198190e34cccd5765",
            "68220979df3542b3969e202a7f681707",
            "1fc017a1495a467486c01ecb9935a0cb",
            "3faba563909a499c8dacdbccf3301d3c",
            "c8b161d8a22e45769f864086ef4b8659",
            "933d34f1e67d47c8a04aaf1d2e72854b"
          ]
        },
        "outputId": "9dfc568d-829f-488f-fcc0-5dd120d11e8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "452e0900bcb6419ab13dfda79bfc624e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"  # Or another model from the paper\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True, token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "st_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "def get_pred_indexes(pred, dataset) :\n",
        "  pred_indexes = []\n",
        "  for ind, pred in enumerate(pred) :\n",
        "    choices = dataset[ind]['choices']\n",
        "    pred_embedding = st_model.encode([pred], normalize_embeddings=True)\n",
        "    choice_embeddings = st_model.encode(choices, normalize_embeddings=True)\n",
        "    similarities = cosine_similarity(pred_embedding, choice_embeddings)[0]\n",
        "    # Find the most similar sentence\n",
        "    most_similar_idx = int(np.argmax(similarities))\n",
        "    pred_indexes.append(most_similar_idx)\n",
        "  return pred_indexes"
      ],
      "metadata": {
        "id": "rkiddOOHuogK"
      },
      "id": "rkiddOOHuogK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base"
      ],
      "metadata": {
        "id": "hh4W93F3rM2H"
      },
      "id": "hh4W93F3rM2H"
    },
    {
      "cell_type": "code",
      "source": [
        "%%skip\n",
        "def predict(question, options):\n",
        "\n",
        "    # Format options\n",
        "    options_str = \"\\n\".join([f\"{option}\" for ind, option in enumerate(options)])\n",
        "\n",
        "    instructions = \"Answer the multiple choice question.\"\n",
        "\n",
        "    # Create prompt\n",
        "    augmented_prompt = f\"{instructions}\\nQuestion: {question}\\nAnswer choices:\\n{options_str}\\n\\nYour Answer:\"\n",
        "\n",
        "    # Generate response - move inputs to the same device as model\n",
        "    inputs = tokenizer(augmented_prompt, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}  # Move inputs to model's device\n",
        "\n",
        "    # Generate only new tokens (the answer)\n",
        "    prompt_length = inputs[\"input_ids\"].shape[1]\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Extract only the newly generated tokens (the answer part)\n",
        "    answer_tokens = outputs[0][prompt_length:]\n",
        "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
        "    # print('answer', answer)\n",
        "    answer_option = answer.strip(). split('\\n')[0]\n",
        "    return answer_option"
      ],
      "metadata": {
        "id": "9Z6utz-OrQwO"
      },
      "id": "9Z6utz-OrQwO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%skip\n",
        "query = \"How have we been able to construct detailed maps of surface features on Venus?\"\n",
        "options = [\"by studying Venus from Earth with powerful optical telescopes\",\"by landing spacecraft on the surface for close-up study\",\"by studying Venus with powerful optical telescopes on spacecraft that were sent to orbit Venus\",\"by using radar from spacecraft that were sent to orbit Venus\"]\n",
        "response = predict(query, options)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "H1dqSnvisDnd"
      },
      "id": "H1dqSnvisDnd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%skip\n",
        "validation = json.load(open('dataset/mmlu_datasets/mmlu_astronomy_validation.json'))\n",
        "test = json.load(open('dataset/mmlu_datasets/mmlu_astronomy_test.json'))"
      ],
      "metadata": {
        "id": "vmAUsMX_ueau"
      },
      "id": "vmAUsMX_ueau",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%skip\n",
        "from tqdm import tqdm\n",
        "val_pred = []\n",
        "val_actual = [item['answer'] for item in validation]\n",
        "for item in tqdm(validation, desc='Validating'):\n",
        "  pred = predict(item['question'], item['choices'])\n",
        "  val_pred.append(pred)"
      ],
      "metadata": {
        "id": "TMsb8xfkvWCe"
      },
      "id": "TMsb8xfkvWCe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%skip\n",
        "val_pred[0:5]"
      ],
      "metadata": {
        "id": "xh94yF9RwWXb"
      },
      "id": "xh94yF9RwWXb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%skip\n",
        "val_pred_indexes = get_pred_indexes(val_pred, validation)\n",
        "val_pred_indexes, val_actual"
      ],
      "metadata": {
        "id": "bdn6s8LMufmn"
      },
      "id": "bdn6s8LMufmn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%skip\n",
        "accuracy = (np.array(val_pred_indexes) == np.array(val_actual)).mean()\n",
        "accuracy"
      ],
      "metadata": {
        "id": "_zYC-9eAwiZ9"
      },
      "id": "_zYC-9eAwiZ9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%skip\n",
        "data = []\n",
        "for ind, item in enumerate(validation):\n",
        "    # Extract the question, choices, and answer index\n",
        "    question = item['question']\n",
        "    choices = item['choices']\n",
        "    answer_idx = item['answer']\n",
        "\n",
        "    # Create a dictionary for this row\n",
        "    row = {\n",
        "        'question': question,\n",
        "        'option_0': choices[0],\n",
        "        'option_1': choices[1],\n",
        "        'option_2': choices[2],\n",
        "        'option_3': choices[3],\n",
        "        'answer_idx': answer_idx,\n",
        "        'predicted_idx': val_pred_indexes[ind]\n",
        "    }\n",
        "\n",
        "    # Add to our data list\n",
        "    data.append(row)\n",
        "df = pd.DataFrame(data)\n",
        "df.head()\n",
        "df.to_csv('base_validation_results.csv', index=False)"
      ],
      "metadata": {
        "id": "FWfzG-fA3Xc9"
      },
      "id": "FWfzG-fA3Xc9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%skip\n",
        "test_pred = []\n",
        "test_actual = [item['answer'] for item in test]\n",
        "for item in tqdm(test, desc='Validating'):\n",
        "  pred = predict(item['question'], item['choices'])\n",
        "  test_pred.append(pred)"
      ],
      "metadata": {
        "id": "9vsCbSlywz1o"
      },
      "id": "9vsCbSlywz1o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%skip\n",
        "test_pred_indexes = get_pred_indexes(test_pred, test)\n",
        "test_pred_indexes[0:5], test_actual[0:5]"
      ],
      "metadata": {
        "id": "TFACYWrE21-M"
      },
      "id": "TFACYWrE21-M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%skip\n",
        "accuracy = (np.array(test_pred_indexes) == np.array(test_actual)).mean()\n",
        "accuracy"
      ],
      "metadata": {
        "id": "jEiogomn3G5T"
      },
      "id": "jEiogomn3G5T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%skip\n",
        "import pandas as pd\n",
        "\n",
        "data = []\n",
        "for ind, item in enumerate(test):\n",
        "    # Extract the question, choices, and answer index\n",
        "    question = item['question']\n",
        "    choices = item['choices']\n",
        "    answer_idx = item['answer']\n",
        "\n",
        "    # Create a dictionary for this row\n",
        "    row = {\n",
        "        'question': question,\n",
        "        'option_0': choices[0],\n",
        "        'option_1': choices[1],\n",
        "        'option_2': choices[2],\n",
        "        'option_3': choices[3],\n",
        "        'answer_idx': answer_idx,\n",
        "        'predicted_idx': test_pred_indexes[ind]\n",
        "    }\n",
        "\n",
        "    # Add to our data list\n",
        "    data.append(row)\n",
        "df = pd.DataFrame(data)\n",
        "df.head()\n",
        "df.to_csv('base_test_results.csv', index=False)"
      ],
      "metadata": {
        "id": "rWLwMQVr3Oor"
      },
      "id": "rWLwMQVr3Oor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG"
      ],
      "metadata": {
        "id": "fGuDMjZc30bO"
      },
      "id": "fGuDMjZc30bO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7304ff94",
      "metadata": {
        "id": "7304ff94"
      },
      "outputs": [],
      "source": [
        "def rag(question, options, k=3):\n",
        "    # Generate query embedding and normalize\n",
        "    question_embedding = embedding_model.encode([question], normalize_embeddings=True)\n",
        "\n",
        "    # Retrieve top-k similar chunks\n",
        "    scores, indices = index.search(question_embedding, k)\n",
        "\n",
        "    # Construct context from retrieved chunks\n",
        "    context = \"\\n\\nSample \".join([chunks[idx] for idx in indices[0]])\n",
        "\n",
        "    # Format options\n",
        "    options_str = \"\\n\".join([f\"{option}\" for ind, option in enumerate(options)])\n",
        "\n",
        "    instructions = \"Answer the multiple choice question at the end.\"\n",
        "\n",
        "    # Create prompt\n",
        "    augmented_prompt = f\"{instructions}\\n\\n-----Beginning of Context------:\\n{context}\\n------End of Context-------\\n\\n\\nQuestion to Answer: {question}\\nAnswer Choices:\\n{options_str}\\n\\nWhat is your Answer?\"\n",
        "\n",
        "    # Generate response - move inputs to the same device as model\n",
        "    inputs = tokenizer(augmented_prompt, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}  # Move inputs to model's device\n",
        "\n",
        "    # Generate only new tokens (the answer)\n",
        "    prompt_length = inputs[\"input_ids\"].shape[1]\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Extract only the newly generated tokens (the answer part)\n",
        "    answer_tokens = outputs[0][prompt_length:]\n",
        "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
        "    answer_option = answer.strip().split('\\n')[0]\n",
        "    return answer_option\n",
        "    try:\n",
        "      if answer_option[0] in ['1','2','3','4'] : return int(answer_option[0]) - 1\n",
        "    except:\n",
        "      print(answer_option)\n",
        "    return answer_option\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How have we been able to construct detailed maps of surface features on Venus?\"\n",
        "options = [\"by studying Venus from Earth with powerful optical telescopes\",\"by landing spacecraft on the surface for close-up study\",\"by studying Venus with powerful optical telescopes on spacecraft that were sent to orbit Venus\",\"by using radar from spacecraft that were sent to orbit Venus\"]\n",
        "response = rag(query, options)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "KJPP2CdXmzjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "936781ed-3bfd-439f-a194-4ac05045a384"
      },
      "id": "KJPP2CdXmzjo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: by using radar from spacecraft that were sent to orbit Venus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation = json.load(open('dataset/mmlu_datasets/mmlu_astronomy_validation.json'))\n",
        "test = json.load(open('dataset/mmlu_datasets/mmlu_astronomy_test.json'))"
      ],
      "metadata": {
        "id": "TyExp1ZAn_hO"
      },
      "id": "TyExp1ZAn_hO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred = []\n",
        "val_actual = [item['answer'] for item in validation]\n",
        "for item in tqdm(validation, desc='Validating'):\n",
        "  pred = rag(item['question'], item['choices'], k=3)\n",
        "  val_pred.append(pred)"
      ],
      "metadata": {
        "id": "pG_I3bQ4uN_Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f978a9c9-df84-4c39-c8ba-2875f0ba68b1"
      },
      "id": "pG_I3bQ4uN_Q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 16/16 [01:45<00:00,  6.57s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUq8RYLy5TED",
        "outputId": "8f0b8e09-2e26-46c2-d389-988583548dcc"
      },
      "id": "jUq8RYLy5TED",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Power emitted is 1/16 times as high; peak emission wavelength is 2 times longer.',\n",
              " 'Answer: The minimization of gravitational potential energy.',\n",
              " \"Answer: Large impacts fractured the Moon's lithosphere allowing lava to fill the impact basins.\",\n",
              " 'Answer: Its rotation axis is nearly perpendicular to the plane of the Solar System.',\n",
              " 'Answer: The answer is 5 : 1.']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred_indexes = get_pred_indexes(val_pred, validation)\n",
        "val_pred_indexes, val_actual"
      ],
      "metadata": {
        "id": "fL1VXE1auwhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cca353c-7cd1-48e9-8679-bab3898379fb"
      },
      "id": "fL1VXE1auwhb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([3, 1, 3, 0, 2, 1, 3, 3, 0, 0, 1, 0, 3, 1, 3, 0],\n",
              " [3, 1, 3, 0, 2, 1, 3, 3, 0, 0, 1, 0, 2, 1, 3, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = (np.array(val_pred_indexes) == np.array(val_actual)).mean()\n",
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t1kwf1e5jIa",
        "outputId": "76a804d4-89e5-454b-d51c-eaa1579353ec"
      },
      "id": "_t1kwf1e5jIa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.9375)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for ind, item in enumerate(validation):\n",
        "    # Extract the question, choices, and answer index\n",
        "    question = item['question']\n",
        "    choices = item['choices']\n",
        "    answer_idx = item['answer']\n",
        "\n",
        "    # Create a dictionary for this row\n",
        "    row = {\n",
        "        'question': question,\n",
        "        'option_0': choices[0],\n",
        "        'option_1': choices[1],\n",
        "        'option_2': choices[2],\n",
        "        'option_3': choices[3],\n",
        "        'answer_idx': answer_idx,\n",
        "        'predicted_idx': val_pred_indexes[ind]\n",
        "    }\n",
        "\n",
        "    # Add to our data list\n",
        "    data.append(row)\n",
        "df = pd.DataFrame(data)\n",
        "df.head()\n",
        "df.to_csv('rag_validation_results.csv', index=False)"
      ],
      "metadata": {
        "id": "D11z0KWV5hQb"
      },
      "id": "D11z0KWV5hQb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = []\n",
        "test_actual = [item['answer'] for item in test]\n",
        "for item in tqdm(test, desc='Testing'):\n",
        "  pred = rag(item['question'], item['choices'], k=3)\n",
        "  test_pred.append(pred)"
      ],
      "metadata": {
        "id": "Kx5PLxBux-_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48c6f2f8-331a-428a-b94d-b8d800109481"
      },
      "id": "Kx5PLxBux-_M",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTesting:   0%|          | 0/152 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Testing: 100%|██████████| 152/152 [19:04<00:00,  7.53s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred_indexes = get_pred_indexes(test_pred, test)\n",
        "test_pred_indexes[0:5], test_actual[0:5]"
      ],
      "metadata": {
        "id": "ue6867IjzY8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68222c52-b233-4a52-8397-241a9a8e8045"
      },
      "id": "ue6867IjzY8_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 3, 2, 1, 0], [0, 3, 2, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = (np.array(test_pred_indexes) == np.array(test_actual)).mean()\n",
        "accuracy"
      ],
      "metadata": {
        "id": "1v0yL2wNyQ7K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11f44b17-1711-4895-8bfa-8adfbae8177c"
      },
      "id": "1v0yL2wNyQ7K",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.6118421052631579)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = []\n",
        "for ind, item in enumerate(test):\n",
        "    # Extract the question, choices, and answer index\n",
        "    question = item['question']\n",
        "    choices = item['choices']\n",
        "    answer_idx = item['answer']\n",
        "\n",
        "    # Create a dictionary for this row\n",
        "    row = {\n",
        "        'question': question,\n",
        "        'option_0': choices[0],\n",
        "        'option_1': choices[1],\n",
        "        'option_2': choices[2],\n",
        "        'option_3': choices[3],\n",
        "        'answer_idx': answer_idx,\n",
        "        'predicted_idx': test_pred_indexes[ind]\n",
        "    }\n",
        "\n",
        "    # Add to our data list\n",
        "    data.append(row)\n",
        "df = pd.DataFrame(data)\n",
        "df.head()\n",
        "df.to_csv('rag_test_results.csv', index=False)"
      ],
      "metadata": {
        "id": "azLuS8Vb-3qf"
      },
      "id": "azLuS8Vb-3qf",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "452e0900bcb6419ab13dfda79bfc624e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc8bb6ddbfb84d04a1d1e232ab4af3d2",
              "IPY_MODEL_6cff29d0e06446eaa3d4c8c80eafd8a5",
              "IPY_MODEL_ae359486e20a4f8da832221b906c6679"
            ],
            "layout": "IPY_MODEL_6308b1005afa4269b3c750ebbd15643d"
          }
        },
        "cc8bb6ddbfb84d04a1d1e232ab4af3d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_531a7b08732d43a198190e34cccd5765",
            "placeholder": "​",
            "style": "IPY_MODEL_68220979df3542b3969e202a7f681707",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6cff29d0e06446eaa3d4c8c80eafd8a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fc017a1495a467486c01ecb9935a0cb",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3faba563909a499c8dacdbccf3301d3c",
            "value": 3
          }
        },
        "ae359486e20a4f8da832221b906c6679": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8b161d8a22e45769f864086ef4b8659",
            "placeholder": "​",
            "style": "IPY_MODEL_933d34f1e67d47c8a04aaf1d2e72854b",
            "value": " 3/3 [01:15&lt;00:00, 24.96s/it]"
          }
        },
        "6308b1005afa4269b3c750ebbd15643d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "531a7b08732d43a198190e34cccd5765": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68220979df3542b3969e202a7f681707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1fc017a1495a467486c01ecb9935a0cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3faba563909a499c8dacdbccf3301d3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8b161d8a22e45769f864086ef4b8659": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "933d34f1e67d47c8a04aaf1d2e72854b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}